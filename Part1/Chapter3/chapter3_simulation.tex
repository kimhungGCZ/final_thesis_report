\section{Interoperation in IoT}
\subsection{Related works}

To increase the interoperability in IoT, researchers have leveraged several approaches and technologies from other fields such as semantic, cloud computing, fog computing, etc. In this section, we present an overview of current approaches as well as challenges to achieve interoperability. For each proposal, we examine its interoperability level (technical, syntactical, semantic and organizational interoperability), openness, and security perspectives.

\subsubsection{Adapters/gateways}

Gateways or adapters are the class of schemes aiming to improve interoperability between IoT devices. This approach uses an intermediate tool called mediator installed in IoT gateways or adapters to converse protocols, data, standards between sending and receiving devices. For instance, an IoT device using Bluetooth connectivity desires connecting to other devices using ZigBee connectivity. In such context, gateway can be installed dedicated hardware or software which could converse Bluetooth connectivity to ZigBee connectivity and vice versa.\\

The most critical challenge of this approach in scalability. With a large number of heterogeneous IoT devices, it demands considerable efforts to design specific connectors. In addition, the performance of conversion must be considered. For example, if the system needs to support n connectivity, we have to develop $\frac{n*(n-1)}{2}$ connectors. These connectors cannot be implemented in a single gateway. Therefore, several one-to-many protocol gateways solution is used. \\

Several proposals in both academic and industrial focus on designing and standardizing IoT gateway. Ponte~\cite{PonteM2M76:online} presents a framework enabling data exchange between various IoT devices through different connectors. The main limitation of this framework is that it only supports HTTP, CoAP, and MQTT, which are incompatible with resource-constrained devices. In addition, developing a new connector is extremely complex works. Zhu et al.~\cite{zhu2010iot} propose an IoT gateway architecture using user-space programmable software to interoperate between wireless sensor network (WSN)\index{WSN} protocols and mobile communication networks or Internet. Such gateway supports data forwarding, protocol conversion, and management. It could be installed on smartphone. However, it does not support accessing collected data via simple API. Using the same method, the authors of~\cite{fantacci2014short} presents a gateway architecture adapting to the differences in device protocols and security issues. But, its scalability does not mention. Leveraging the computation power of smartphone, ~\cite{pereira2016iot}\cite{aloi2017enabling} build a mobile gateway supporting the same functionality with IoT gateway. The main limitation of this approach is the heavy energy consumption. The Semantic Gateway as a Service (SGS)\index{SGS} is presented in~\cite{asensio2014protocol} as an intelligent gateway providing semantic interoperability for IoT system. The raw sensor data is transmitted to center gateway via proxy layer, which supports multi-protocol. Then, the data is adding semantic annotations defined by W3C SSN ontology. This step provides the semantic operation for collected data. 

\subsubsection{Virtual networks}

The first concept of virtual network is presented in~\cite{hoebeke2011managed} aiming to integrate wireless sensor network to the Internet for end-to-end communication. The main idea of this approach is that a virtual network is built on the top of physical network enabling end-to-end communication using different protocols. Then, the application developer could interact with devices, sensor, or actuator in physical network via virtual network. This concept is also used to develop Internet of Things Virtual Network (IoT-VN)\index{IoT-VN}~\cite{ishaq2012internet}. The interoperation is achieved by integrating all heterogeneous IoT devices into the same virtual network. This solution targets both normal and constrained devices. However, integrating all IoT devices into IoT-VN is impossible due to the fragment of IoT market. In addition, the virtual network can not ensure the scalability with massive IoT devices. 

\subsubsection{Networking technologies}
Several networking protocols and technologies have been proposed to ensure the interoperability in the networking layer in IoT. In this section, we present the main solutions achieving this goal.\\

\textbf{\textit{Software-defined networking (SDN)\index{SDN}: }} SDN is a new networking paradigm enabling efficient network configuration in order to make the current wireless and mobile networks more intelligent, efficient, secure~\cite{kreutz2015software}. Relied on these advantages, SDN is used to handle the massive amount of collected data in IoT. Matinez and Skarmeta~\cite{bizanis2016sdn} used SDN to enable communication between IoT devices using IPv6. They also added an IoT controller over SDN controller to facilitate the device management operations. Thus, even if the devices have different protocol, the controller could convert it to be compatible with the destination. In~\cite{qin2014software}, the authors provide a middleware containing a layered IoT SDN controller to manage heterogeneity in IoT multi-network. They use a central controller for monitoring and coordinating the existing devices and data flows in the system. However, the current SDN technologies are hard to support traditional networking devices. Therefore, we need to have novel solutions to abstract the traditional networking devices in SDN regardless of their specific hardware configurations~\cite{8017556}.\\

\textbf{\textit{Network function virtualization (NFV)\index{NFV}: }} The network function virtualization is used to separate the physical hardware of network from the software layer running on them. This allows creating several services on the same physical hardware. The authors in~\cite{li2015general} introduce a general SDN-IoT framework by combining IoT architecture with SDN architecture. This consists of (1) APIs layer for developing IoT application, (2) middle layer containing distributed network OS, SDN controller, (3) lowest layer containing SDN switches and IoT gateway. However, the limitations of NSV is the complexity and security issues while maintaining the network OS. In addition, virtualization has many challenges in term of resource management, operation, and security.\\

\textbf{\textit{Fog computing: }} The integration between Internet of Things, Cloud computing, and Fog computing arises a novel concept named Fog of Things, where the computing, storage, and networking services are placed at the edge of the network rather than centralized cloud servers~\cite{prazeres2016soft}. The raw data collected from IoT devices converses into usable knowledge before being available on Web. This also facilitates interoperability in IoT and reduces the network latency. In this fog layer, collected data is semantically annotated for further applications to be interoperable~\cite{gyrard2017building}.

\subsubsection{Open API }
API\index{API} is an interface written in a high-level language. This interface is used to access data or functions of an application. Thus, to enable interoperability, the API has to be well-documented and open for all developers. Most of current IoT platforms provide a publish API based on RESTful principles, and allow common operations such as PUT, GET, PUSH, or DELETE to support developers access their services. However, these APIs are designed as platform-specific or proprietary. This leads to heterogeneity in the syntax and API operations. For example, an IoT application is used to control the air conditioner. This application could increase temperature via an API provided by air conditioner provider. If the application desires to control air conditioner of other providers, without standard API, the developer must write new dedicated code for new providers. However, with a standard API, the application only simply change the end-point (new air conditioner address). Therefore, a standard API promises to enable cross-platform interoperability. \\

With the rapid growth of IoT platform along with lacking standard for API, the large number of diverse API have been created. This raises the issues relating to developing applications and API heterogeneity in IoT. To bridge these gaps, HyperCat provides a specification enabling syntactic interoperability between different APIs and services, which could be described under a Catalog format~\cite{lea2013hypercat}. The resources in a catalog are identified by URI and tagged with metadata. On the other hand, the Big-IoT European projects have been working on a generic interworking API which allows accessing to resources of all existing IoT platforms. This API is expected to enable syntactic and cross-platform interoperability~\cite{BIGIoT–B21:online}. The interworking API acts as an adapter which needs to be implemented by other platforms.

\subsubsection{Service oriented architecture (SOA)\index{SOA}}

To enable device and cross-platform interoperability, the researchers have built Service Oriented Architecture on the top of network layer~\cite{vinoski2003integration} so that the devices and data are effectively managed via service components~\cite{vinoski2003integration}\cite{li2014distributed}. In detail, the functionality and operations of devices are wrapped by these components. Thereby, IoT application could simply expose the device resources via these services that will significantly increase the interoperability of both network and device if we can standardize them. The authors in \cite{papazoglou2007service} applied the web service technologies to SOA to maximize service sharing and interoperability. In particular, \cite{alam2010semantic} using classic web service-oriented approach (WS-* web service) and \cite{varga2017making} using resource-oriented approach (REST web services) aim to increase syntactic interoperability. Pautasso et al~\cite{pautasso2008restful} compared the benefit of WS-* web service and REST web services to SOA in various use-cases. They claim that REST web services are suitable for tactical integration over the Web, whereas WS-* web service fits for enterprise applications.

\subsubsection{Semantic web technologies}

The Semantic Web technologies are designed to semantically describe Web resources. Currently, many research directions leverage the benefit of Semantic Web technologies into IoT to achieve semantic interoperability. The most common paradigm of such integration is Semantic Web of Things ~\cite{scioscia2009building} for common understanding of IoT data and entities (services, devices, etc.). This goal is achieved by using shared standards, vocabulary in a schema form or an ontology to describe the information in IOT. \\

On the Semantic Web of Things, Ontologies (or vocabularies) define the concepts or relationships used to describe and represent an area of concern~\cite{fensel2001ontologies}. They are used to describe information when ambiguities or heterogeneity may exist on the terms between different domains. Many ontologies have been proposed such as W3C Semantic Sensor Network (SSN)~\cite{compton2012ssn}, SAREF~\cite{daniele2015created}, and OpenIoT~\cite{soldatos2015openiot}. A study of the existing ontologies in several domains is presented in~\cite{ganzha2017semantic}. They also describe in detail how using ontologies could achieve cross-platform interoperability. However, there is not global ontological standard, most of existing ontologies are domain-specific.\\

Several IoT research project aims to leverage the benefits of ontologies or other semantic technologies to enhance interoperability in IoT. Semantic Sensor Web (SSW)\index{SSW}~\cite{sheth2008semantic} is the adoption of Sensor Web and Semantic Web technology. SensorML~\cite{botts2007opengis} provide by Open Geospatial Consortium (OGC)\index{OGC} is XML-based standard to describe sensor of web. UbiROAD~\cite{terziyan2010ubiroad} introduces a framework enabling semantic interoperability at data level and functional protocol level. Serrano~\cite{gyrard2016connected} analyzes the current semantic interoperability challenges in IoT. Base on this analysis, the authors provide a methodology named SEG to achieve semantic interoperability at application layer. Their method is to add semantic annotation to heterogeneous IoT data to assist developers in building IoT applications. In another way, the authors of~\cite{de2011service} presents a set of semantic models for describing IoT components. They also introduce a novel concept named ``sensing as a service'' that supports access to IoT resources and functions through standard services.

\subsection{Open Challenges}

Although many IoT solutions including standards, platform have been proposed to achieve interoperability in IoT, there are still some remaining challenges relating this topic. In this section, we will present major challenges in IoT interoperability based on reviewed solutions

\begin{itemize}

    \item Most of the reviewed solutions focus on solving interoperability issue from a specific perspective rather than a comprehensive solution. The solutions tend to provide device and network interoperability so that there is lack of cross-platform or cross-domain interoperability solutions. With the huge benefit from semantic technologies and open API, the room for future work in this area is obviously substantial.
    
    \item IoT devices are the key element in IoT system. Thus, addressing interoperability for the devices is vital for the success of IoT. Due to the lack of a communication standard, integrating a heterogeneous device into an IoT platform regardless of its communication technology, hardware or configuration is still a major challenge. Some solutions rely on network entity like a gateway. However, these solutions limit the scalability and flexibility to deal with the significant growth of IoT devices in both quantity and type. Furthermore, direct communication between two heterogeneous devices is still unresolved issue in IoT.
    
    \item Most of the IoT platforms are designed using cloud-based model so that they can not be deployed on edge entities for speed and efficiency. Therefore, a lightweight IoT platform deployed either on cloud or edge entities is missing. 
    
    \item The current IoT platforms provide an open APIs to access their services. However, these APIs are designed from custom RESTful principles and data model. Thereby, cross-platform interoperability is very challenging. 
    
    \item Enabling cross-platform interoperability between different platforms must be considered the differences in technologies, underlying features and provided services. In addition, the integration should not require the major changes in the platform. 
    
    \item Various academia, industry, and standardization address the IoT interoperability by providing the standards. However, it does not mean that the proposed standards will be globally accepted and used. Thus, it probably emerges the heterogeneity in IoT standard. 
    
    \item There is no method for testing the interoperability. Currently, evaluating the efficiency of interoperation solution requires a face-to-face meeting. Thus, an automation method for interoperability testing
    
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Reliability in IoT}

\subsection{Related work}

In the Internet of Things context, most of the data collected from IoT Things is inherent uncertainty and inconsistency. Thus, data cleaning is a crucial step, along with data acquisition and data mining, to gain the success of IoT paradigms or services. It also helps the IoT application developer more focus on the core solutions than the prior process to ensure data reliability. In general, data cleaning consists of two main steps: (1) Outlier detection: identifying the errors or events in data, (2) data repairing: repairing the identified errors. Data cleaning is not new in IoT context. There are several works in both academy and industry aiming to effectively clean the IoT data. In this section, we briefly review common data cleaning approaches. 

\paragraph{Outlier detection}

\textbf{\textit{Unsupervised methods: }} One of the most common unsupervised outlier detection approaches is used the concept of nearest neighbor analysis. Such techniques detect the abnormality by examining the distance or similarity between two data instances. Normal data instances occur in dense neighborhoods, while anomalies occur far from their closet neighbors \cite{chandola2009anomaly}. The distance (or similarity) can be calculated in a different way. For example: Euclidean distance is the wide usage for continuous attributes in \cite{Breunig:2000:LID:335191.335388}\cite{tang2001robust}\cite{kriegel2009loop}. For categorical attributes, a simple matching coefficient is often used but more complex distance measures can also be used \cite{boriah2008similarity}\cite{chandola2008understanding}. The original idea of nearest neighbor anomaly detection defines the anomaly score of a data instance as its distance to its $ k^{th} $ nearest neighbor in a given data set which is mentioned in \cite{byers1998nearest}. This work also has been applied to detect shorted turns in wind turbine-generators in \cite{guttormsson1999elliptical}. The basic technique is enhanced by researchers in various aspects. For example: The author in \cite{eskin2002geometric}\cite{angiulli2002fast}\cite{zhang2006detecting} calculates anomaly score from the sum of the distance to k nearest neighbors. \cite{knorr1997unified} counts the number of nearest neighbor within d distance as anomaly score. \cite{wu2006outlier} designs a simple sampling technique to reduces the complexity of algorithm. A Resolution Outlier Factor (ROF) was proposed in \cite{fan2006nonparametric}. According to this method, points are outliers or within a cluster depends on the resolution of applied distance thresholds. Another approach is based on the idea: “An instance that lies in a neighborhood with low density is declared to be anomalous while an instance that lies in a dense neighborhood is declared to be normal”. The most well-known algorithm in such technique is Local Outlier Factor (LOF)\cite{Breunig:2000:LID:335191.335388}. For given data instance, the anomaly score is basically the ratio of the average local densities of its k-nearest neighbors over the local density itself.  However, LOF ineffectively detects the regions which are not clearly separated. Several researches were subsequently proposed to extend the concept of LOF. The authors in \cite{jin2006ranking} uses the symmetric nearest neighbor relationship to define the outlier score. \cite{tang2002enhancing} introduces a new variation of LOF named Connectivity-based Outlier Factor (COF)\cite{tang2001robust} which can detect the abnormality distributed on arbitrarily shaped clusters. The LOF is also combined with other techniques. For example, the work in \cite{he2002outlier}\cite{he2003discovering} calculate the anomaly score, named Cluster-Based Local Outlier Factor (CBLOF), from local distances to nearby cluster and the size of the clusters. A more recent proposal is presented in \cite{wang2011statistical} using relative entropy as the distance measurement. \\

\textbf{\textit{Supervised methods: }} Supervised anomaly detection methods require the training data in which correctly labels both normal and abnormal data. Several approaches using supervise leaning has been proposed such as MetaCost~\cite{domingos1999metacost} uses a relabeling approach to classification. The general idea of this method is to relabel some data point in training dataset by using a cost so that normal data point will have a reasonable probability to be classified into abnormal data. This work aims to make the training data set more balanced. Support vector learning has been applied for outlier detection such as one-class learning~\cite{scholkopf2001learning} and support vector data description~\cite{tax2004support}. The behind idea is to learn a boundary that encloses the normal data so that all outside data instances is considered as outliers. Other approach is to use the transitional machine learning classifier to classify the dataset into normal and abnormal classes such as the Bayes classifie~\cite{zadrozny2003cost}, nearest-neighbor classifier~\cite{mani2003knn}, decision trees~\cite{ting2002instance}\cite{weiss2003learning}, rule-based classifiers~\cite{joshi2001mining}\cite{juszczak2003uncertainty} and SVM classifiers~\cite{tang2009svms}\cite{wu2003class}. There are two major issues that arise in supervised anomaly detection. 
\begin{itemize}
    \item The number of abnormal data in training dataset is extremely minor in comparison with normal data. This makes the model imbalance in class distribution and reduces the detection quality. Several approaches have been addressed this issue~\cite{joshi2001computational}\cite{chawla2004special}\cite{phua2004minority}.
    \item Obtaining training data including accurate and representative label data is usually challenging. Thus, the authors in~\cite{theiler2003resampling}\cite{steinwart2005classification} propose the techniques used to inject synthetic anomalies to the normal data set. Their goal is to manipulate the real scenario as much as possible. 
\end{itemize}

\paragraph{Data repairing }

Smoothing-based Data repairing is a light-weight and simple technique used for online data repairing. For example, the simple moving average (SMA)\index{SMA}~\cite{brillinger1981time} calculates the value of current points from the mean of the last k points. Instead of using unweighted mean, the exponentially weighted moving average (EWMA)\index{EWMA}~\cite{gardner2006exponential} multiples this mean with a weight value, which is exponentially decreasing over the time. Other approach named SWAB smoothing~\cite{keogh2001online} uses regression function to online-repairing of stream data. Based on last sliding windows, SWAB approximates the trend of data by linear interpolation function. Despite the lightweight and simple implementation, these smoothing methods have very low repair accuracy. In addition, they also change the originally normal points in repairing process. \\

Constraint-based Data Repairing techniques repairs data based on given constraints, while minimizing the repair modification~\cite{bohannon2005cost}\cite{chu2013holistic}. The SCREEN algorithm~\cite{song2015screen}
works under the assumption that the speed of data changes (namely speed constraint) is constrained. This means the $jump$ of values is limited and considered as an anomaly if it is out of a given boundary. Based on such assumption, they propose a solution for stream data to identify and repair the  ``jump'' values in a given sequence (windows data) w.r.t the speed constraint while minimizing the repair distance. To achieve this goal, the authors introduce a novel concept named ``Median Principle'' to find the middle point of a specific sequence that is intuitively believed minimizing the repair distance. In addition, to deal with the tradeoff between choosing the window size and speed constraints. They proposed an adaptive windows size based recently extreme speed constraint (min or max speed). However, the SCREEN can show high performance in repairing single anomaly but hardly handle a collective anomaly. In addition, the repairing results strongly rely on the correctness of initial assumption. Being aware of such limitations, a latter algorithm named Iterative Minimum Repairing (IMR)\index{IMR}~\cite{zhang2017time} is proposed. The general idea of such algorithm is that combining between labeling some dirty observations and iterative repairing from high to low confidence repairs could enhance the performance. After each iteration, the parameter of the ARX\index{ARX}~\cite{park2005outlier} model is calculated to generate the repair candidate list based on the difference between original and inferred data. The data point with the minimum difference is repaired. The stop conditions of repairing procedure could be reaching the threshold of convergence or maximum number of iterations. 

\subsection{Open Challenges}

Ensuring the data quality, especially in the IoT context has been facing many challenges. Most of the solutions are dedicated to specific purposes such as uniquely detect anomaly or clean data. There is no comprehensive solution for identifying to repairing the error. Apart from that, current data cleaning solutions are still lack of:
\begin{itemize}
    \item \textit{Scalability: } With the exponential growth of IoT, the data collected for IoT devices is massive. The advantage of unsupervised approaches is light-weight, simple. But, their accuracy is very low. In contrast, supervised approaches could offer high accuracy but they require a huge amount of time to train the model. Therefore, we need a solution that ensures scalability and flexibility while maintaining high accuracy.
    
    \item \textit{Heterogeneity of data sources: } IoT data could be collected from various data sources (e.g. sensors, devices, RFID tags, etc.). The data cleaning techniques should be able to analyze and combine the relations of various data sources to increase the accuracy. In addition, the proposed technique has to handle different variables, which describe end-user interests. Depending on the user-cases, user requires different detection quality. For example, fleet management application strongly requires high accuracy in discriminating the sensor errors and filling tank event while monitoring applications such as CO2 or temperature do not require such level of accuracy. This requirement leads to a new challenge on how to satisfy user's desired quality.
    
    \item \textit{Distinguish error and event} The major feature that missing from all data cleaning technique is the ability to distinguish outliers caused by error from those caused by events. Current data cleaning technique is often applied to filter out dirty data. This means that the detected points are discarded as useless noises. Unfortunately, the eliminated data may contain notable events, also known as {\em change points}. These changes occur by accident (e.g., a fire in a forest) or because of human intervention (e.g., watering the tree). Preserving these events is essential to interpret the context. For example, to optimize the watering schedule, a city environment management company deploys the sensors to monitor the impact of watering on soil humility under the trees. However, a significant increase in soil humility due to water can be detected as anomaly and removed from the data~\cite{kang2013prevention}. Thus, the ability to explicitly distinguish between anomalies and events is needed. 
    
\end{itemize}


\section{Device Reliability in IoT}

\subsection{Related works}
\subsection{Open Challenges}
